---
authors: [rubenfiszel]
tags: ['Launch week', 'Workflow Engine']
image: ./vscode_extension.png
slug: launch-week-1/fastest-workflow-engine
description: 'Why is windmill the fastest self-hostable workflow engine and job processing framework'
---

# Fastest self-hostable open-source workflow engine

## A more scoped crown

Before you raise your pitchforks, let's unpack our claim a bit, we are not claiming to be necessarily faster than your specialized,
hand-built workflow engine written on top of the amazing [BEAM](<https://en.wikipedia.org/wiki/BEAM_(Erlang_virtual_machine)>)
(from which, we took inspiration) but rather only among the "all-inclusive", self-hostable workflow engines. We recognize 3 main ones today:

- Airflow
- Prefect
- Temporal

There are tons of workflow engines, but not many of them are generic enough to support arbitrary workloads of jobs defined in code,
and even those have restrictions: Airflow and Prefect only supports Python. Windmill on the other hand supports: Typescript/Javascript, Python, Go, Bash and direct SQL queries to BigQuery, Snowflake, Mysql, Postgresql.
Some are notoriously hard to write for (because of complex SDKs, looking at you Airflow's XCOM or Temporal idempotency primitives) and deploy to.
Windmill offers an integrated DX that allows to build and test workflows in a few minutes and interactively in a mix of raw code for the steps and low-code (or YAML) for the DAG itself.
One benefit of being very fast is that it makes running tests very fast too both in terms of latency to start and to run. Wasting time waiting for previews and tests to run is not fun.

## Should temporal even be there?

Temporal doesn't actually manage the [workers but only the tasks queues](https://docs.temporal.io/workers). So even after having written your Temporal workflow, you will still need to manage your workers separately.
To some degree, temporal is not a workflow engine but a specialized durable execution engine. Windmill also support reactivity (aka waiting for event) and can be qualified as a durable execution engine as well.
That being said, temporal is amazing at what it does and if there are overlap between windmill and temporal, there are clearly use-cases where you should use temporal rather than windmill,
as the backbone of your micro-services async patterns at the scale of Uber for instance. On the other hand, sending arbitrary jobs to an internal cluster is out-of-scope for temporal as you will still need to painfully deploy "Worker Programs" beforehand.

::: info

We leave analytics/ETL engine such as Spark or Dagster out of it for today as they are not workflow engines _per se_ even if they are built on top of ones.

ETL and analytics workflows will be covered later this week, but you will find that windmill is also extremely performant for analytics workloads.

:::

## Workflow Engine vs Job queues

Job Queues are at the core of Workflow Engines and constitute the crux of any background job processing.
There are already plenty of great queues implementation under the form of managed services (SQS), distributed scalable services (kafka) and software (e.g: Redis with rmsq) or libraries (Orban).
They are mostly sufficient to use by themselves and many developers will find satisfaction avoiding a workflow engine
altogether by building their own logic around a job queue. This is akin to

And here is the benchmark data and dedicated benchmarking post to go along with it.

BENCHMARKING BLOG POST

Here we will focus on the why, the architecture of windmill and the different components of a workflow engine aka orchestration engine

## What is a workflow engine, and what constitute an "all-inclusive" workflow engine

First some definitions, a workflow is a directed acyclic graph [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of nodes that represent job specifications.
A workflow engine is a distributed system that takes a workflow and orchestrate it to completion on workers while respecting among others all the dependency constraints of each job.
There exists a great variety of kind of workflows and many softwares are domain specific workflow engine and workflow specs (if you are a software engineer, you probably already wrote one without realizing it).

What will interest us here are workflows that can run arbitrary code in at least one major programming language (Python/Typescript/Go/Bash). Those are the most generic but also the most complex and the most difficult to optimize.
Every node is code that takes in argument and data from other steps or the flow inputs, do some side effects (http requests, write to disk/s3) and then return some data for other steps to consume.

5 Major benefits of workflow engines:

- Resource allocation: Clusters can be well leveraged and every job can be assigned to different workers with different resources (cpu, memory, gpus) and guarantee that the full resource of the worker will be available to the job
- Parralelism: When the constraints of a workflow allow some steps to be ran in parallel (sub-branches, for-loop), a workflow engine can dispatch those steps on multiple physically separate workers and not just threads
- Observability: every job has a unique id and can be observed separately: the inputs, logs, outputs, status can be inspected
- Durability: Machine dies, side effects fail for unexpected reasons. Workflows need to be restartable as close from the unexpected events. One way of achieving this is idempotency: a single operation is the same as the effect of making several identical operation. When in doubt, replay the entire flow without any consequences.
  This is usually implemented with a log file and an sdk that skip the side effect when the unique id attached to an operation is part of the log.
  Another way is transactional snapshotting of the flow state, store the state after each operation. To resume, simply reload that last state and execute from there. Windmill does the latter and assume that idempotency can be implemented when desired in userland.
- Reactivity: Suspend the flow until it is resumed again based on an event such as a webhook or an approval

Additionally, an all-inclusive workflow engine should dynamically register new available workers and make it easy to deploy new workflow, assign different jobs to different workers, monitor the health of the workers and of the system itself.
Additionally, a developer platform on top of a workflow engine should handle permissions such that different users with different level of permissions can run different workflows and those workflows to have access to
different resources depending on the roles of the caller.

## So why is windmill very fast

In a workflow engine, the efficiency depend:

- efficiency to compute transitions, the new jobs to schedule given the last job that completed, and the efficiency of the workers themselves to pull scheduled jobs and run them
- efficiency to pass data between steps
- efficiency of the worker to pull job, start executing the code, and then submit result and new sate

Windmill is extremely fast because on all 3 aspects, it uses a simple design, optimized everywhere, that leverage maximally both Postgresql and Rust.

## System design and Queue

Windmill offers a single binary (compiled from Rust) than can run both as the api server, the worker or both.
Both workers and servers are connected to Postgresql but not to each other.
The servers only expose the api and the frontend. The queue is implemented in PG itself.
Jobs can be triggered externally by calling the api which will push new jobs to the queue.

Jobs are stored in 2 tables in PG:

- queue (while the job is not completed, even when running)
- completed_job

Jobs are not removed from the queue upon start, but their field `running` is set to true. Queue is implemented with the traditional `UPDATE SKIP LOCKED`

```sql
UPDATE queue
SET running = true
, started_at = coalesce(started_at, now())
, last_ping = now()
, suspend_until = null
WHERE id = (
    SELECT id
    FROM queue
    WHERE running = false AND scheduled_for <= now() AND tag = ANY($1)
    ORDER BY priority DESC NULLS LAST, scheduled_for, created_at
    FOR UPDATE SKIP LOCKED
    LIMIT 1
)
RETURNING *
```

This is as fast as you can get with Postgresql, as long as every field is properly indexed. Some variation of this delete the job from the queue instead of updating a flag, or set a time at which it is invisible for others to be pulled. It's all the same principle under the hood.

When a flow job is pushed, its inputs, pointers to an immutable flow definition is pushed alongside the job and initial flow state (see under) is pushed within the job (it's a big row!)

A worker will then pick the flow job, read the flow definition and flow state, realize that it needs to push the first step in the queue and that's it. This is the initial transition of the flow.

The workers pull jobs one at a time, run them to completion, and progres the state if the job is part of a flow job (which is a separate job). The servers themselves do no orchestration, every flow progress is run by the worker themselves.

## States

Workflow engines represents jobs as a finite state machine [(FSM)](https://en.wikipedia.org/wiki/Finite-state_machine). The 4 main states commonly used are:

- Waiting for pre-requisites (right events like a webhook to have been received, or all the depenency jobs to have been completed)
- Pre-requisite fulfilled, waiting for a worker to pull job and execute
- Running
- Completed (with Success or Failure)

Other states are usually some refinement on the 4 above

In windmill the full flow itself is a finite state machine, where both the [spec of the flow](https://github.com/windmill-labs/windmill/blob/main/backend/windmill-common/src/flows.rs#L196) of the [flow status](https://github.com/windmill-labs/windmill/blob/main/backend/windmill-common/src/flow_status.rs#L28) is an easy to read struct.

A flow state is wholly defined by a step counter and an array of flow module states. Some flow module states are more complex such as the ones corresponding to the for-loop and branches. One interesting aspect to note is that subflows like branches or for-loop iterations are their own well defined jobs which have a pointer to the flow that has triggered them as `parent_job`.

Hence, branches and Forloops are special kind of flow states that include an array of ids that point to all the subflows that have been launched.

What is extremely convenient about this design is that **every state transaction is just one transactionned postgresql statement**

## Data passing

TODO

## Worker efficiency

TODO

## Dedicated worker for flows
